---
title: "spatial modeling decisions"
format: html
editor: visual
execute: 
  error: TRUE
editor_options: 
  chunk_output_type: console
---

Spatial lag model: i believe this is very similar to what we've already done with temporal lag.... the dependent variable in neighboring areas is included - use this when the value of variable in one county directly affects the value in another county (we do not necessarily believe that the health of a county DIRECTLY impacts the health of neighboring county)

Spatial error model: capture spatial autocorr in the error term - use this when omitted spatially correlated vars cause dependence (perhaps by ignoring the presence of highways, commuting zones, etc we are introducing spatial dependence which we can remove using a spatial error model)

Sparks and Sparks tested both spatial lag and spatial error models and concluded that spatial error models better described mortality data - therefore, that's what we'll use too. i see no need to double check this since we do not believe that counties directly affect their neighbors, as we would assume with spatial lag)

-   In non spatial models we used the following syntax to specify random intercepts and slopes: (1\|GEOID) is a random intercept; each fips's reg line is shifted up/down randomly with mean 0

-   (1+t\|GEOID) includes a random slope for time; the effect of time differs randomly from fips to fips

It appears that in the splm package, should instead use:

-   model = ("within", "random", "pooling") to add random effects

-   effect = c("individual", "time", "twoways") to tell the function where the random effect belongs

-   effect = "individual" -\> spatial fixed effects

-   effect = "time" -\> time fixed effects

The help page for this particular function is not useful. Instead, I am referencing this document: https://www.insee.fr/en/statistiques/fichier/3635545/imet131-k-chapitre-7.pdf

From this document we see that:

"the user may choose between pooling for a pooled data model, within for a fixed-effect model or random for a randomised model. It is also possible to define whether the effects relate to individuals or/and periods using the option effects that can be established as equal to individual, time or twoways. We can also choose whether the specification includes spatial terms: lag=T in the SAR model, or lag=F in all other cases. Lastly, it is possible to choose the nature of the specification in the random effects model: spatial.error="b"for a Baltagi specification, spatial.error="kkp"for the KKP-style specification (Kapoor et al. 2007) or spatial.error="none" in all other cases."

Based on this, in order to match the models in lmer that we were using previously, using the spml function, we will need model = "random", effect = "twoways", lag = FALSE, and spatial.error = "none"

Switching spatial.error = "none" should be the only change from the nonspatial to the spatial

I will test this on a subset of states so things run quickly

```{r}
library(tidyverse)
library(splines)
library(splm)

# get data 
load("data_processed/migterm_imp.RData")

# add geography for use on spatial stuff 
cp = tidycensus::get_acs(geography = "county", year = 2019, variables = c(tpop = "B01003_001"), survey = "acs5", output = "wide", geometry = TRUE)

cpmig = merge(cp, migterm_imp, by.x = "GEOID", by.y = "destid", all.y = TRUE)


cpmig = cpmig %>% 
 arrange(year) %>% 
 group_by(GEOID) %>% 
 mutate(t = row_number())

cpmig$ft = as.factor(cpmig$t)

# these are the steps for wrangling all contiguous US states, not just a subset 

cpall = cpmig %>% select(rate_d1, ft, rate_d0, migterm, GEOID, geometry, ft,t)

spall = cpall %>% select(GEOID, geometry) %>% distinct()
queen = spdep::poly2nb(spall, row.names = "GEOID", queen = T)
queenw = spdep::nb2listw(queen, style = "W", zero.policy = TRUE)

cpall = plm::pdata.frame(cpall, index = c("GEOID", "ft"))
cpall$ft = factor(cpall$ft, levels = c(1:9))



######################################################
# subset of WI, MN, IL, and IA 

cpsub = cpmig %>% group_by(GEOID) %>% 
  filter(statecode %in% c(55,27,17,19)) %>% #wi, mn, il, ia
  select(rate_d1, ft, rate_d0, migterm, GEOID, geometry, ft,t)

spsub= cpsub %>% select(GEOID, geometry) %>% distinct()
queensub = spdep::poly2nb(spsub, row.names = "GEOID", queen = T)
queensubw = spdep::nb2listw(queensub, style = "W", zero.policy = TRUE)

cpsub = plm::pdata.frame(cpsub, index = c("GEOID", "ft"))
cpsub$ft = factor(cpsub$ft, levels = c(1:9))


#this formula is similar to the best found using previous approach 
fff4 = rate_d1 ~ ft + ns(rate_d0, df = 4) + ns(migterm, df = 4) #+ (1 + t | GEOID) 

fffnospline = rate_d1~ ft + n
# this is with the plm package rather than the splm package since there is no spatial effect... need to confirm that this is the same as rigging lmer package myself 

# note that model = "random" throws an error due to only 8 times the data.... this may not be a problem on a full dataset. 
m1nospat = plm::plm(formula = fff4, #listw = queenw, 
                    data = cpall, model = "random", effect = "twoways") #, lag = FALSE, spatial.error = "none")
```

Looks like this approach won't work... throws error on full dataset and on subset of data. stack overflow suggests changing the re estimation method to "walhus", "amemiya", or "nerlove" (need to do more reading to understand these methods.....) https://stats.stackexchange.com/questions/531445/random-effects-number-of-coefficients-more-then-individuals

```{r}
m1nospat = plm::plm(formula = fff4, #listw = queenw, 
                    data = cpall, model = "random", effect = "twoways", random.method = "walhus") #, lag = FALSE, spatial.error = "none")

summary(m1nospat)

# m1nospat = plm::plm(formula = fff4, listw = queenw, 
#                     data = cpall, model = "random", effect = "twoways", random.method = "walhus", lag = FALSE, spatial.error = "none")


```

This returns theta values of 0 indicating no panel effect??? perhaps this is because all panel effects are taken care of in the saturated model? and/or because they're misspecified? .. if i use the plm function, perhaps each year does not need to be specified as a separate factor? Perhaps the original saturated model is not what we really want and instead we want a simple timeseries model that can then be extended to a spatial model. this feels more inline with the examples I have seen online... and if we choose to use the splm package to account for spatial autocorrelation, then i think it is best to use the plm package to account for spatial correlation since splm is a direct extension of plm making it easier to compare spatial and nonspatial models.

I feel extra confident in the switch to a panel model with autoreg term after asking the following of chatgpt:

*describe the differences between an autoregressive panel data model and a fully saturated model with each year as a separate factor*

***ChatGPT:***

***An autoregressive panel data model and a fully saturated model with each year as a separate factor represent different approaches to modeling panel data and capturing the temporal dynamics within the dataset. Let's explore the key differences between the two:***

1.  ***Autoregressive Panel Data Model:***

    -   ***Purpose: Autoregressive models aim to capture the temporal dependencies or autocorrelation within each entity's time-series data.***

    -   ***Specification: In autoregressive panel data models, lagged values of the dependent variable are included as explanatory variables to model the relationship between the variable at a given time and its past values.***

    -   ***Example (using `plm` in R):***

        ```         
        RCopy code
        ```

        ***`ar_model <- plm(y ~ lag(y, 1) + x1 + x2, data = pdata, model = "pooling")`***

    -   ***Advantage: Efficiently captures time-series dependencies and helps control for autocorrelation within each entity's observations over time.***

2.  ***Fully Saturated Model with Each Year as a Separate Factor:***

    -   ***Purpose: This model aims to capture the variation across different years by treating each year as a separate factor.***

    -   ***Specification: Dummy variables for each year are included in the model, allowing for a unique intercept for each year. Each year is treated as a distinct category.***

    -   ***Example (using `lm` in R):***

        ```         
        RCopy code
        ```

        ***`saturated_model <- lm(y ~ factor(year) + x1 + x2, data = panel_data)`***

    -   ***Advantage: Allows for capturing year-specific effects, but the model may become less efficient as the number of years increases due to the large number of parameters.***

***Key Differences:***

-   ***Focus on Temporal Dynamics:***

    -   ***Autoregressive panel data models focus on modeling the temporal dynamics within each entity's time series.***

    -   ***Fully saturated models with separate year factors primarily focus on capturing the variation across different years, treating them as distinct categories.***

-   ***Parameterization:***

    -   ***Autoregressive models include lagged values of the dependent variable, introducing a smaller set of parameters to capture temporal dependencies.***

    -   ***Fully saturated models introduce a separate parameter for each year, potentially leading to a large number of parameters, especially with a long time series.***

-   ***Efficiency and Flexibility:***

    -   ***Autoregressive models may be more efficient in capturing temporal dependencies efficiently.***

    -   ***Fully saturated models may become less efficient as the number of years increases, and they might not capture time-series patterns as explicitly.***

***In practice, the choice between these models depends on the specific research question and the characteristics of the data. Autoregressive models are more tailored for capturing time-series dynamics, while fully saturated models with separate year factors are more focused on capturing year-specific effects. Researchers need to consider the trade-offs in terms of model complexity and efficiency.***

Since we want to add spatial autocorrelation as well, i do think efficiency is a concern! And since I have already shown above that a fully saturated model cannot be estimated using the default setting of the plm function, I feel confident about moving away from the previous fully saturated model.

Below is a baseline panel model without migration and without spatial autocorrelation:

```{r}

subcp = cpall %>% select(geometry, t, GEOID, migterm, rate_d1) %>% 
  filter(!is.na(migterm)) %>% distinct()
subcp = plm::pdata.frame(subcp, index = c("GEOID", "t"))


m1nospat = plm::plm(formula = rate_d1 ~ rate_d0 , #listw = queenw, 
                    data = cpall, model = "random", effect = "twoways") #, lag = FALSE, spatial.error = "none")
```

Now a model with migration but without spatial correlation

```{r}
m1mig = plm::plm(formula = rate_d1 ~ rate_d0 + migterm , #listw = queenw, 
                    data = cpall, model = "random", effect = "twoways") #, lag = FALSE, spatial.error = "none")
```

Finally a model with migration and spatial correlation

(testing on a subset due to potential size issues)

```{r}
m1migspat = splm::spml(formula = rate_d1 ~ rate_d0 + migterm , listw = queensubw, data = cpsub, model = "random", effect = "twoway", lag = FALSE, spatial.error = "b")

```

I believe this error is due to size.....

both individual and time have extremely long run times... and if they each run separately, there seems no reason that they couldn't run together other than a built in break point. I'm stuck. Thought that the panel data approach would alleviate some of these space issues. Need to go back to HTCS.

------------------------------------------------------------------------

THIS IS ANOTHER PRIOR ATTEMPT THAT I DON'T WANT TO LOSE IN CASE WE REVERT, BUT THIS IS NOT CURRENTLY IN USE BECAUSE IT USES A SATURATED MODEL RATHER THAN A PANEL MODEL

Using Queen contiguity because it looks like that's what's used by Sparks and Sparks and also by the county-level examples in this course that I referenced as a baseline before accounting for temporal correlation too https://crd230.github.io/lab8.html#Spatial_error_model

To account for both space and time, I use the splm package in R.

First I test the models using a subset of WI data.

The splm package requires a balanced dataset (ie all destinations must have the same years of available data). This requires throwing out or making up some data :(

We stick with ML - includes random effects, more about modeling than estimation

```{r WIonly }

# cp = tidycensus::get_acs(geography = "county", year = 2019, variables = c(tpop = "B01003_001"), survey = "acs5", state = "WI", output = "wide", geometry = TRUE)
# 
# wisub = mnn %>% filter(substr(destid, 1, 2) == "55")
# 
# nyears = wisub %>% group_by(destid) %>% 
#   count()
# 
# #there are two counties with 7 years of data only....
# # is it better to remove the two counties or remove the 8th year....
# # easier to remove the two counties because it turns out that they don't have the same 7 years of data 
# 
# wisub_bal = wisub %>% group_by(destid) %>% 
#   filter(n()==8)
# 
# cpmnn = merge(cp, wisub_bal, by.x = "GEOID", by.y = "destid")
# 
# 
# spsub= cpmnn %>% select(GEOID, geometry) %>% distinct()
# queen = spdep::poly2nb(spsub, row.names = "GEOID", queen = T)
# queenw = spdep::nb2listw(queen, style = "W", zero.policy = TRUE)
# 
# library(splm)
# fm = rate_d1 ~ ft + rate_d0 + migterm
# 
# 
#   
# wmod = splm::spml(formula = fm, data = wisub_bal, model = "random", 
#                   listw = queenw, lag = TRUE)

```

```{r, include = FALSE}

# library(tidyverse)
# library(splines)
# library(splm)
# 
# # potential formulas that i'd like to test (with and without spatial error)
# 
# fm = rate_d1 ~ ft + rate_d0 + migterm
# fnom = rate_d1 ~ ft + rate_d0 
# fff2 = rate_d1 ~ ft + ns(rate_d0, df = 2) + ns(migterm, df = 2) + (1 + t | GEOID) 
# fff4 = rate_d1 ~ ft + ns(rate_d0, df = 4) + ns(migterm, df = 4) # + (1 + t | GEOID) 
# fff3 = rate_d1 ~ ft + ns(rate_d0, df = 3) + ns(migterm, df = 3) # + (1 + t | GEOID) 
# fff5 = rate_d1 ~ ft + ns(rate_d0, df = 5) + ns(migterm, df = 5) # + (1 + t | GEOID) 
# fff2_nomig = rate_d1 ~ ft + ns(rate_d0, df = 2) # + (1 + t | GEOID) 
# fff4_nomig = rate_d1 ~ ft + ns(rate_d0, df = 4) # + (1 + t | GEOID) 
# fff3_nomig = rate_d1 ~ ft + ns(rate_d0, df = 3) # + (1 + t | GEOID) 
# fff5_nomig = rate_d1 ~ ft + ns(rate_d0, df = 5) # + (1 + t | GEOID) 
# 
# 
# # get a subset of data 
# load("data_processed/migterm_imp.RData")
# 
# cp = tidycensus::get_acs(geography = "county", year = 2019, variables = c(tpop = "B01003_001"), survey = "acs5", output = "wide", geometry = TRUE)
# 
# cpmig = merge(cp, migterm_imp, by.x = "GEOID", by.y = "destid", all.y = TRUE)
# 
# 
# cpmig = cpmig %>% 
#  arrange(year) %>% 
#  group_by(GEOID) %>% 
#  mutate(t = row_number())
# 
# cpmig$ft = as.factor(cpmig$t)
# 
# 
# cpbal = cpmig %>% group_by(GEOID) %>% 
#   filter(statecode %in% c(55,27,17,19)) %>% #wi, mn, il, ia
#   select(rate_d1, ft, rate_d0, migterm, GEOID, geometry, ft,t)
# 
# 
# 
# spsub= cpbal %>% select(GEOID, geometry) %>% distinct()
# queen = spdep::poly2nb(spsub, row.names = "GEOID", queen = T)
# queenw = spdep::nb2listw(queen, style = "W", zero.policy = TRUE)
# 
# cpbal = plm::pdata.frame(cpbal, index = c("GEOID", "ft"))
# cpbal$ft = factor(cpbal$ft, levels = c(1:9))
# 
# #should follow similar proceedure for spatial as for nonspatial 
# # compare with and without spatial, with and without migterm, with and without splines 
# 
# #####################################################################
# 
# # linear mixed effect model with no spatial component via maximum likelihood 
# fff0 = rate_d1 ~ ft + ns(rate_d0, df = 2) + ns(migterm, df = 2) + (0 + t| GEOID) 
# fff1 = rate_d1 ~ ft + ns(rate_d0, df = 2) + ns(migterm, df = 2) + (1 + t| GEOID) 
# rfff0 = lme4::lmer(formula = fff0, data = cpbal)
# rfff1 = lme4::lmer(formula= fff1, data = cpbal) #this fails to converge…. 
# # not sure what the difference is between 0 and 1 
# 
# 
# 
# # spatial panel mixed effect model via maximum likelihood  
# fff2 = rate_d1 ~ ft + ns(rate_d0, df = 2) + ns(migterm, df = 2) #+ (1 + t | GEOID) 
# sfff2 = splm::spml(formula = fff2, data =cpbal, model = "random", effect = "individual", listw = queenw, lag = FALSE, spatial.error= "none")
# summary(sfff2)
# 
# # regular model with what i think are random effects using the spgm function 
```

switch to just a lagged model, no more saturated model

add covariates like sparks and sparks? how does sparks and sparks account for temporality with their covariates? are covars for the year prior?
